import torch
import torch.nn as nn
import torch.nn.functional as F

from nets.resnet import resnet50
from nets.vgg import VGG16

# ğŸ”¥ å¯¼å…¥æ³¨æ„åŠ›æœºåˆ¶
from atention import CAA, EMA, EfficientAdditiveAttnetion, AFGCAttention, DualDomainSelectionMechanism, AttentionTSSA
from module.ECA import ECA_layer
from atention import C2f_IEL

# ğŸ”¥ å¯¼å…¥CAA_HSFPNæ¨¡å—
from simplified_block import CAA_HSFPN

# å®šä¹‰UNetçš„ä¸Šé‡‡æ ·æ¨¡å—
class unetUp(nn.Module):
    def __init__(self, in_size, out_size, attention_type='none'):
        super(unetUp, self).__init__()
        
        # ç¬¬ä¸€ä¸ªå·ç§¯å±‚
        self.conv1 = nn.Conv2d(in_size, out_size, kernel_size=3, padding=1)
        # ç¬¬äºŒä¸ªå·ç§¯å±‚
        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size=3, padding=1)
        # ä¸Šé‡‡æ ·æ“ä½œï¼Œæ”¾å¤§ç‰¹å¾å›¾
        self.up = nn.UpsamplingBilinear2d(scale_factor=2)
        self.relu = nn.ReLU(inplace=True)
        
        # ğŸ”¥ æ­£ç¡®è®¡ç®—æ³¨æ„åŠ›æ¨¡å—çš„è¾“å…¥é€šé“æ•°
        if in_size == 3072:  # up_concat4
            skip_channels = 1024  # feat4
            up_channels = 2048    # feat5_up
        elif in_size == 1024:  # up_concat3
            skip_channels = 512   # feat3
            up_channels = 512     # up4
        elif in_size == 512:   # up_concat2
            skip_channels = 256   # feat2
            up_channels = 256     # up3
        elif in_size == 192:   # up_concat1
            skip_channels = 64    # feat1
            up_channels = 128     # up2
        else:
            # ğŸ”¥ é€šç”¨è®¡ç®—æ–¹å¼
            up_channels = out_size * 2
            skip_channels = in_size - up_channels
            
            if skip_channels <= 0:
                skip_channels = out_size
                up_channels = in_size - skip_channels
        
        # ğŸ”¥ æ·»åŠ æ³¨æ„åŠ›æ¨¡å—
        if attention_type != 'none':
            self.attention = DecoderAttentionModule(
                skip_channels=skip_channels,
                up_channels=up_channels,
                attention_type=attention_type
            )
        else:
            self.attention = None
            
        print(f"ğŸ”§ unetUpæ¨¡å—: in_size={in_size}, out_size={out_size}, skip_channels={skip_channels}, up_channels={up_channels}, attention={attention_type}")

    def forward(self, inputs1, inputs2):
        # inputs1: è·³è·ƒè¿æ¥ç‰¹å¾
        # inputs2: æ¥è‡ªä¸‹å±‚çš„ç‰¹å¾ï¼Œéœ€è¦ä¸Šé‡‡æ ·
        
        # ğŸ”¥ ä¸Šé‡‡æ ·inputs2
        up_feat = self.up(inputs2)
        
        # ğŸ”¥ åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.attention is not None:
            skip_enhanced, up_enhanced = self.attention(inputs1, up_feat)
        else:
            skip_enhanced = inputs1
            up_enhanced = up_feat
        
        # ğŸ”¥ æ‹¼æ¥å¢å¼ºåçš„ç‰¹å¾
        outputs = torch.cat([skip_enhanced, up_enhanced], 1)
        
        # ğŸ”¥ å·ç§¯å¤„ç†
        outputs = self.conv1(outputs)
        outputs = self.relu(outputs)
        outputs = self.conv2(outputs)
        outputs = self.relu(outputs)
        
        return outputs

# ğŸ”¥ æ–°å¢ï¼šç‰¹å¾èåˆæ¨¡å—ï¼Œå°†CAA_HSFPNé›†æˆåˆ°ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´
class EncoderDecoderBridge(nn.Module):
    """ç¼–ç å™¨-è§£ç å™¨æ¡¥æ¥æ¨¡å—ï¼Œä½¿ç”¨CAA_HSFPNè¿›è¡Œç‰¹å¾èåˆ"""
    
    def __init__(self, backbone='resnet50', use_caa_hsfpn=True):
        super(EncoderDecoderBridge, self).__init__()
        
        self.use_caa_hsfpn = use_caa_hsfpn
        self.backbone = backbone
        
        if use_caa_hsfpn:
            if backbone == 'resnet50':
                # ResNet50çš„å„å±‚ç‰¹å¾é€šé“æ•°ï¼šfeat1(64), feat2(256), feat3(512), feat4(1024), feat5(2048)
                self.caa_hsfpn_feat1 = CAA_HSFPN(ch=64, flag=True)    # æœ€æµ…å±‚ç‰¹å¾
                self.caa_hsfpn_feat2 = CAA_HSFPN(ch=256, flag=True)   # ç¬¬äºŒå±‚ç‰¹å¾
                self.caa_hsfpn_feat3 = CAA_HSFPN(ch=512, flag=True)   # ç¬¬ä¸‰å±‚ç‰¹å¾
                self.caa_hsfpn_feat4 = CAA_HSFPN(ch=1024, flag=True)  # ç¬¬å››å±‚ç‰¹å¾
                self.caa_hsfpn_feat5 = CAA_HSFPN(ch=2048, flag=True)  # æœ€æ·±å±‚ç‰¹å¾ï¼ˆç“¶é¢ˆå±‚ï¼‰
                
            elif backbone == 'vgg':
                # VGG16çš„å„å±‚ç‰¹å¾é€šé“æ•°ï¼ˆæ ¹æ®å®é™…VGGå®ç°è°ƒæ•´ï¼‰
                self.caa_hsfpn_feat1 = CAA_HSFPN(ch=64, flag=True)
                self.caa_hsfpn_feat2 = CAA_HSFPN(ch=128, flag=True)
                self.caa_hsfpn_feat3 = CAA_HSFPN(ch=256, flag=True)
                self.caa_hsfpn_feat4 = CAA_HSFPN(ch=512, flag=True)
                self.caa_hsfpn_feat5 = CAA_HSFPN(ch=512, flag=True)
            
            print(f"ğŸ”¥ CAA_HSFPNæ¡¥æ¥æ¨¡å—å·²å¯ç”¨ - éª¨å¹²ç½‘ç»œ: {backbone}")
            print(f"   - å°†å¯¹æ‰€æœ‰ç¼–ç å™¨ç‰¹å¾è¿›è¡Œç©ºé—´åæ ‡æ³¨æ„åŠ›å¢å¼º")
        else:
            print(f"âš ï¸ CAA_HSFPNæ¡¥æ¥æ¨¡å—å·²ç¦ç”¨")
    
    def forward(self, encoder_features):
        """
        å¯¹ç¼–ç å™¨ç‰¹å¾åº”ç”¨CAA_HSFPNå¢å¼º
        Args:
            encoder_features: [feat1, feat2, feat3, feat4, feat5] ç¼–ç å™¨è¾“å‡ºçš„5å±‚ç‰¹å¾
        Returns:
            enhanced_features: å¢å¼ºåçš„ç‰¹å¾åˆ—è¡¨
        """
        feat1, feat2, feat3, feat4, feat5 = encoder_features
        
        if self.use_caa_hsfpn:
            # ğŸ”¥ å¯¹æ¯å±‚ç‰¹å¾åº”ç”¨CAA_HSFPNç©ºé—´åæ ‡æ³¨æ„åŠ›å¢å¼º
            feat1_enhanced = self.caa_hsfpn_feat1(feat1)  # å¢å¼ºæµ…å±‚ç‰¹å¾çš„ç©ºé—´ç»†èŠ‚
            feat2_enhanced = self.caa_hsfpn_feat2(feat2)  # å¢å¼ºç¬¬äºŒå±‚ç‰¹å¾
            feat3_enhanced = self.caa_hsfpn_feat3(feat3)  # å¢å¼ºç¬¬ä¸‰å±‚ç‰¹å¾
            feat4_enhanced = self.caa_hsfpn_feat4(feat4)  # å¢å¼ºç¬¬å››å±‚ç‰¹å¾
            feat5_enhanced = self.caa_hsfpn_feat5(feat5)  # å¢å¼ºç“¶é¢ˆå±‚ç‰¹å¾çš„è¯­ä¹‰è¡¨è¾¾
            
            return [feat1_enhanced, feat2_enhanced, feat3_enhanced, feat4_enhanced, feat5_enhanced]
        else:
            # ä¸ä½¿ç”¨CAA_HSFPNï¼Œç›´æ¥è¿”å›åŸç‰¹å¾
            return [feat1, feat2, feat3, feat4, feat5]

# å®šä¹‰UNetä¸»å¹²ç½‘ç»œ
class Unet(nn.Module):
    def __init__(self, num_classes=9, pretrained=False, backbone='resnet50', 
                 attention_type='caa', layer_attentions=None, use_c2f_iel=True, 
                 use_caa_hsfpn=True, use_transmamba=False):  # ğŸ”¥ æ–°å¢å‚æ•°
        super(Unet, self).__init__()
        
        # ğŸ”¥ ä»…æ·»åŠ è¿™ä¸¤è¡Œä¿å­˜å‚æ•°çŠ¶æ€
        self.use_caa_hsfpn = use_caa_hsfpn
        self.use_c2f_iel = use_c2f_iel
        self.use_transmamba = use_transmamba
        # ğŸ”¥ å¤„ç†å¤šå±‚æ³¨æ„åŠ›é…ç½®
        if layer_attentions is None:
            self.layer_attentions = {
                'up_concat4': 'caa',
                'up_concat3': 'eca',# 'eca'
                'up_concat2': 'none',
                'up_concat1': 'none'
            }
        else:
            self.layer_attentions = {}
            for layer in ['up_concat4', 'up_concat3', 'up_concat2', 'up_concat1']:
                self.layer_attentions[layer] = layer_attentions.get(layer, attention_type)
        
        print(f"\nğŸ”¥ æ„å»ºå¢å¼ºç‰ˆå¤šå±‚æ³¨æ„åŠ›UNet:")
        print(f"   éª¨å¹²ç½‘ç»œ: {backbone}")
        print(f"   CAA_HSFPNæ¡¥æ¥: {'å¯ç”¨' if use_caa_hsfpn else 'ç¦ç”¨'}")
        print(f"   C2f_IELå¢å¼º: {'å¯ç”¨' if use_c2f_iel else 'ç¦ç”¨'}")
        print(f"   TransMambaå¤„ç†: {'å¯ç”¨' if use_transmamba else 'ç¦ç”¨'}")  # ğŸ”¥ æ–°å¢
        print(f"   è§£ç å™¨æ³¨æ„åŠ›é…ç½®:")
        for layer, att_type in self.layer_attentions.items():
            print(f"     {layer}: {att_type}")
        
        # é€‰æ‹©backbone
        if backbone == 'vgg':
            self.vgg = VGG16(pretrained=pretrained)
            in_filters = [192, 384, 768, 1024]  # å„å±‚è¾“å…¥é€šé“æ•°
        elif backbone == "resnet50":
            self.resnet = resnet50(pretrained=pretrained, use_transmamba=use_transmamba)  # ğŸ”¥ ä¼ é€’å‚æ•°
            in_filters = [192, 512, 1024, 3072]
        else:
            raise ValueError('Unsupported backbone - `{}`, Use vgg, resnet50.'.format(backbone))
        
        out_filters = [64, 128, 256, 512]  # å„å±‚è¾“å‡ºé€šé“æ•°

        # ğŸ”¥ æ·»åŠ ç¼–ç å™¨-è§£ç å™¨æ¡¥æ¥æ¨¡å—ï¼ˆCAA_HSFPNï¼‰
        self.encoder_decoder_bridge = EncoderDecoderBridge(
            backbone=backbone, 
            use_caa_hsfpn=use_caa_hsfpn
        )

        # ğŸ”¥ å®šä¹‰å››ä¸ªå¸¦æ³¨æ„åŠ›çš„ä¸Šé‡‡æ ·æ¨¡å—
        self.up_concat4 = unetUp(
            in_filters[3], out_filters[3], 
            attention_type=self.layer_attentions['up_concat4']
        )
        self.up_concat3 = unetUp(
            in_filters[2], out_filters[2], 
            attention_type=self.layer_attentions['up_concat3']
        )
        self.up_concat2 = unetUp(
            in_filters[1], out_filters[1], 
            attention_type=self.layer_attentions['up_concat2']
        )
        self.up_concat1 = unetUp(
            in_filters[0], out_filters[0], 
            attention_type=self.layer_attentions['up_concat1']
        )

        # resnet50 backboneä¸‹çš„é¢å¤–ä¸Šé‡‡æ ·å·ç§¯
        if backbone == 'resnet50':
            self.up_conv = nn.Sequential(
                nn.UpsamplingBilinear2d(scale_factor=2), 
                nn.Conv2d(out_filters[0], out_filters[0], kernel_size=3, padding=1),
                nn.ReLU(),
                nn.Conv2d(out_filters[0], out_filters[0], kernel_size=3, padding=1),
                nn.ReLU(),
            )
        else:
            self.up_conv = None

        # æœ€åä¸€å±‚1x1å·ç§¯è¾“å‡ºç±»åˆ«æ•°
        self.final = nn.Conv2d(out_filters[0], num_classes, 1)

        self.backbone = backbone

        # ğŸ”¥ æ·»åŠ C2f_IELç‰¹å¾å¢å¼ºæ¨¡å—ï¼ˆåœ¨CAA_HSFPNä¹‹åè¿›ä¸€æ­¥å¢å¼ºï¼‰
        if use_c2f_iel:  # ğŸ”¥ æ”¹ä¸ºæ¡ä»¶åˆå§‹åŒ–
            print("ğŸ”§ åˆå§‹åŒ–C2f_IELæ¨¡å—...")
            if backbone == "resnet50":
                # ResNet50çš„ç‰¹å¾é€šé“æ•°ï¼šfeat1(64), feat2(256), feat3(512), feat4(1024)
                self.c2f_iel_feat1 = C2f_IEL(c1=64, c2=64, n=1, shortcut=False, e=0.5)
                self.c2f_iel_feat2 = C2f_IEL(c1=256, c2=256, n=2, shortcut=False, e=0.5)  
                self.c2f_iel_feat3 = C2f_IEL(c1=512, c2=512, n=3, shortcut=False, e=0.5)
                self.c2f_iel_feat4 = C2f_IEL(c1=1024, c2=1024, n=4, shortcut=False, e=0.5)
            elif backbone == "vgg":
                self.c2f_iel_feat1 = C2f_IEL(c1=64, c2=64, n=1, shortcut=False, e=0.5)
                self.c2f_iel_feat2 = C2f_IEL(c1=128, c2=128, n=1, shortcut=False, e=0.5)  
                self.c2f_iel_feat3 = C2f_IEL(c1=256, c2=256, n=1, shortcut=False, e=0.5)
                self.c2f_iel_feat4 = C2f_IEL(c1=512, c2=512, n=1, shortcut=False, e=0.5)
            print("âœ… C2f_IELæ¨¡å—åˆå§‹åŒ–å®Œæˆ")
        else:
            print("âš ï¸ C2f_IELæ¨¡å—å·²ç¦ç”¨")
            # ğŸ”¥ è®¾ç½®ä¸ºNoneé¿å…è°ƒç”¨é”™è¯¯
            self.c2f_iel_feat1 = None
            self.c2f_iel_feat2 = None
            self.c2f_iel_feat3 = None
            self.c2f_iel_feat4 = None
        
        self.use_c2f_iel = use_c2f_iel
    
    def forward(self, inputs):
        # ğŸ”¥ æ­¥éª¤1: ç¼–ç å™¨ç‰¹å¾æå–
        if self.backbone == "vgg":
            encoder_features = self.vgg.forward(inputs)
        elif self.backbone == "resnet50":
            encoder_features = self.resnet.forward(inputs)

        # ğŸ”¥ æ­¥éª¤2: ç¼–ç å™¨-è§£ç å™¨æ¡¥æ¥ï¼ˆCAA_HSFPNç©ºé—´åæ ‡æ³¨æ„åŠ›å¢å¼ºï¼‰
        enhanced_features = self.encoder_decoder_bridge(encoder_features)
        feat1_bridge, feat2_bridge, feat3_bridge, feat4_bridge, feat5_bridge = enhanced_features

        # ğŸ”¥ æ­¥éª¤3: C2f_IELè¿›ä¸€æ­¥ç‰¹å¾å¢å¼ºï¼ˆåœ¨å‰å››å±‚ï¼‰
        if self.use_c2f_iel:
            feat1_enhanced = self.c2f_iel_feat1(feat1_bridge)  # åŒé‡å¢å¼ºfeat1
            feat2_enhanced = self.c2f_iel_feat2(feat2_bridge)  # åŒé‡å¢å¼ºfeat2  
            feat3_enhanced = self.c2f_iel_feat3(feat3_bridge)  # åŒé‡å¢å¼ºfeat3
            feat4_enhanced = self.c2f_iel_feat4(feat4_bridge)  # åŒé‡å¢å¼ºfeat4
            feat5_final = feat5_bridge  # feat5ä»…ä½¿ç”¨CAA_HSFPNå¢å¼º
        else:
            # ä»…ä½¿ç”¨CAA_HSFPNå¢å¼ºçš„ç‰¹å¾
            feat1_enhanced = feat1_bridge
            feat2_enhanced = feat2_bridge
            feat3_enhanced = feat3_bridge
            feat4_enhanced = feat4_bridge
            feat5_final = feat5_bridge

        # ğŸ”¥ æ­¥éª¤4: è§£ç å™¨é˜¶æ®µï¼ˆä½¿ç”¨åŒé‡å¢å¼ºåçš„ç‰¹å¾ï¼‰
        up4 = self.up_concat4(feat4_enhanced, feat5_final)  # ä½¿ç”¨åŒé‡å¢å¼ºçš„feat4
        up3 = self.up_concat3(feat3_enhanced, up4)          # ä½¿ç”¨åŒé‡å¢å¼ºçš„feat3
        up2 = self.up_concat2(feat2_enhanced, up3)          # ä½¿ç”¨åŒé‡å¢å¼ºçš„feat2
        up1 = self.up_concat1(feat1_enhanced, up2)          # ä½¿ç”¨åŒé‡å¢å¼ºçš„feat1

        # resnet50ä¸‹å†ä¸Šé‡‡æ ·ä¸€æ¬¡
        if self.up_conv != None:
            up1 = self.up_conv(up1)

        # è¾“å‡ºåˆ†å‰²ç»“æœ
        final = self.final(up1)
        
        return final

    # å†»ç»“backboneå‚æ•°ï¼Œä¸å‚ä¸è®­ç»ƒ
    def freeze_backbone(self):
        if self.backbone == "vgg":
            for param in self.vgg.parameters():
                param.requires_grad = False
        elif self.backbone == "resnet50":
            for param in self.resnet.parameters():
                param.requires_grad = False

    # è§£å†»backboneå‚æ•°ï¼Œå‚ä¸è®­ç»ƒ
    def unfreeze_backbone(self):
        if self.backbone == "vgg":
            for param in self.vgg.parameters():
                param.requires_grad = True
        elif self.backbone == "resnet50":
            for param in self.resnet.parameters():
                param.requires_grad = True
    
    def get_attention_summary(self):
        """è·å–æ³¨æ„åŠ›é…ç½®æ‘˜è¦"""
        summary = self.layer_attentions.copy()
        summary['caa_hsfpn_bridge'] = hasattr(self, 'encoder_decoder_bridge') and self.encoder_decoder_bridge.use_caa_hsfpn
        summary['c2f_iel_enhancement'] = self.use_c2f_iel
        return summary

# ğŸ”¥ è§£ç å™¨æ³¨æ„åŠ›æ¨¡å—ä¿æŒä¸å˜
class DecoderAttentionModule(nn.Module):
    """è§£ç å™¨æ³¨æ„åŠ›æ¨¡å—ï¼Œç”¨äºè·³è·ƒè¿æ¥å’Œä¸Šé‡‡æ ·ç‰¹å¾çš„èåˆ"""
    
    def __init__(self, skip_channels, up_channels, attention_type='caa'):
        super(DecoderAttentionModule, self).__init__()
        
        self.attention_type = attention_type
        self.skip_channels = skip_channels
        self.up_channels = up_channels
        
        if attention_type == 'caa':
            self.skip_attention = CAA(ch=skip_channels)
            self.up_attention = CAA(ch=up_channels)
            
        elif attention_type == 'eca':
            self.skip_attention = ECA_layer(skip_channels)
            self.up_attention = ECA_layer(up_channels)
            
        elif attention_type == 'ema':
            self.skip_attention = EMA(channels=skip_channels)
            self.up_attention = EMA(channels=up_channels)
            
        elif attention_type == 'spatial':
            self.skip_spatial_att = nn.Sequential(
                nn.Conv2d(skip_channels, max(skip_channels//8, 1), 1),
                nn.ReLU(inplace=True),
                nn.Conv2d(max(skip_channels//8, 1), 1, 1),
                nn.Sigmoid()
            )
            self.up_spatial_att = nn.Sequential(
                nn.Conv2d(up_channels, max(up_channels//8, 1), 1),
                nn.ReLU(inplace=True),
                nn.Conv2d(max(up_channels//8, 1), 1, 1),
                nn.Sigmoid()
            )
            
        elif attention_type == 'channel':
            self.skip_channel_att = nn.Sequential(
                nn.AdaptiveAvgPool2d(1),
                nn.Conv2d(skip_channels, max(skip_channels//16, 1), 1),
                nn.ReLU(inplace=True),
                nn.Conv2d(max(skip_channels//16, 1), skip_channels, 1),
                nn.Sigmoid()
            )
            self.up_channel_att = nn.Sequential(
                nn.AdaptiveAvgPool2d(1),
                nn.Conv2d(up_channels, max(up_channels//16, 1), 1),
                nn.ReLU(inplace=True),
                nn.Conv2d(max(up_channels//16, 1), up_channels, 1),
                nn.Sigmoid()
            )
            
        elif attention_type == 'none':
            pass
            
        else:
            print(f"âš ï¸ è­¦å‘Š: ä¸æ”¯æŒçš„æ³¨æ„åŠ›ç±»å‹ '{attention_type}'ï¼Œå°†ä½¿ç”¨æ— æ³¨æ„åŠ›æ¨¡å¼")
            self.attention_type = 'none'
        
        if attention_type != 'none':
            print(f"âœ… è§£ç å™¨æ³¨æ„åŠ›æ¨¡å—: {attention_type} (skip:{skip_channels}, up:{up_channels})")
    
    def forward(self, skip_feat, up_feat):
        if self.attention_type == 'caa':
            skip_enhanced = self.skip_attention(skip_feat)
            up_enhanced = self.up_attention(up_feat)
            
        elif self.attention_type == 'eca':
            skip_enhanced = self.skip_attention(skip_feat)
            up_enhanced = self.up_attention(up_feat)
            
        elif self.attention_type == 'ema':
            skip_enhanced = self.skip_attention(skip_feat)
            up_enhanced = self.up_attention(up_feat)
            
        elif self.attention_type == 'spatial':
            skip_att = self.skip_spatial_att(skip_feat)
            up_att = self.up_spatial_att(up_feat)
            skip_enhanced = skip_feat * skip_att
            up_enhanced = up_feat * up_att
            
        elif self.attention_type == 'channel':
            skip_att = self.skip_channel_att(skip_feat)
            up_att = self.up_channel_att(up_feat)
            skip_enhanced = skip_feat * skip_att
            up_enhanced = up_feat * up_att
            
        else:
            skip_enhanced = skip_feat
            up_enhanced = up_feat
        
        return skip_enhanced, up_enhanced
